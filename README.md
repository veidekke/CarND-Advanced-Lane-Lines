# Advanced Lane Finding Project

**Stephan Brinkmann**

The goals / steps of this project are the following:

* Compute the camera calibration matrix and distortion coefficients given a set of chessboard images.
* Apply a distortion correction to raw images.
* Use color transforms, gradients, etc., to create a thresholded binary image.
* Apply a perspective transform to rectify binary image ("birds-eye view").
* Detect lane pixels and fit to find the lane boundary.
* Determine the curvature of the lane and vehicle position with respect to center.
* Warp the detected lane boundaries back onto the original image.
* Output visual display of the lane boundaries and numerical estimation of lane curvature and vehicle position.

[//]: # (Image References)

[undistort_chess]: ./output_images/undistort_chess.jpg
[undistort]: ./output_images/undistort.jpg
[s_binary]: ./output_images/s_binary.jpg
[various_gradients]: ./output_images/various_gradients.jpg
[pre_pipeline_test1]: ./output_images/pre_pipeline_test1.jpg
[pre_pipeline_test7]: ./output_images/pre_pipeline_test7.jpg
[warped]: ./output_images/warped.jpg
[lane_lines]: ./output_images/lane_lines.jpg
[lane_lines_2]: ./output_images/lane_lines_2.jpg
[final]: ./output_images/final.jpg

## [Rubric](https://review.udacity.com/#!/rubrics/571/view) Points
Here I will consider the rubric points individually and describe how I addressed each point in my implementation.  

---
## Writeup / README

#### 1. Provide a Writeup / README that includes all the rubric points and how you addressed each one.  You can submit your writeup as markdown or pdf.  [Here](https://github.com/udacity/CarND-Advanced-Lane-Lines/blob/master/writeup_template.md) is a template writeup for this project you can use as a guide and a starting point.  

You're reading it!

### Camera Calibration

#### 1. Briefly state how you computed the camera matrix and distortion coefficients. Provide an example of a distortion corrected calibration image.

The code for this step is contained in the first and second code cells of the IPython notebook (`p4.ipynb`).  

I read in all calibration images provided in the `/camera_cal/` folder, convert them to grayscale, and extract images points from each of them using the OpenCV function `cv2.findChessboardCorners(...)`.

These points are then appended to a common `imgpoints` array. The `objpoints` array is used to store the coordinates of all corners in the world, following the pattern `(0, 0, 0), (1, 0, 0), (2, 0, 0), ...., (6, 5, 0)`

Both arrays are fed to the `cv2.calibrateCamera(...)` function, which return the camera matrix together with the distance vectors.

The above steps only need to be done once (unless you change the camera, in which case new calibration photos need to be taken).

Using `cv2.undistort(...)`, which I wrap as `undistort(img)`, a random calibration image is undistorted to verify the correctness of the process:

![][undistort_chess]


### Pipeline (single images)

#### 1. Provide an example of a distortion-corrected image.

The first step of the pipeline is undistorting the source image. Code cell 3 takes care of this. It converts the image from BGR to RGB and applies the undistortion.

The result:

![][undistort]

The differences are not extremely apparent, but visible especially around the edges of the image. The undistorted image looks less "fish-eye-y".

#### 2. Describe how (and identify where in your code) you used color transforms, gradients or other methods to create a thresholded binary image.  Provide an example of a binary image result.

I went with a combination of S-channel isolation and thresholding (code cell 4) and x-Sobel gradient (code cell 5).

The former works well on yellow lines, as seen here:

![][s_binary]

The latter is responsible for white lines, as seen here ("Thresholded Gradient (X)"):

![][various_gradients]

In that image, you can also see other methods, gradients, and combinations (code cell 5). Since they do not add a lot to the x-Sobel, I just went with that to keep the pipeline lean and fast.

To confirm that my choice was appropriate, I ran my pre-pipeline (cell 6) on all test images (cell 7), outputting a stacked (to visualize what part each binary image played in the final composite) as well as a combined image. Here are two examples (S-channel in blue, Sobel in green):

![][pre_pipeline_test1]

![][pre_pipeline_test7]

(The test images `test7.jpg` and `test8.jpg` were generated by me from parts of the video that my pipeline originally struggled with.)

#### 3. Describe how (and identify where in your code) you performed a perspective transform and provide an example of a transformed image.

Code cell 8 contains the `perspective_transform(...)` function that warps an image to a bird's-eye view. In code cell 9, I run a test using the binary version of `test1.jpg`:

![][warped]

The red dots are the source and destination points I picked. The following table shows the exact values (starting with the top-left point, going clockwise).

| Source        | Destination   |
|:-------------:|:-------------:|
| 585, 450      | 320, 0        |
| 685, 450      | 960, 0        |
| 1110, 720     | 960, 720      |
| 210, 720      | 320, 720      |

To verify my choice, I checked whether the lane lines in `straight_lines1.jpg` and `straight_lines2.jpg` actually appear as straight lines in their warped counterparts.

#### 4. Describe how (and identify where in your code) you identified lane-line pixels and fit their positions with a polynomial?

Code cells 10-15 contain everything needed to identify and fit lane lines.

Code cell 10 contains factors for conversion between pixels and meters, and a `solve(...)` helper function for solving second degree polynoms is implemented in code cell 11 .

##### Naive approach

The function `find_lane_lines_init(...)` in code cell 12 implements the naive approach of finding and fitting lane lines. It takes the warped binary image as input and returns as necessary fits.

First, a histogram of the bottom half of the image is created, and the left- and right-most parts are cut off, because they led to false-positives, where railings on the very edge of an image would be recognized as the lane lines.

Then, the peaks of the left and right halves of the histogram are identified, and used as a base for the algorithm.

Using 9 sliding windows of width 100 (margin 50), the algorithm tries to "follow" the lanes lines starting from the base points at the bottom of the image all the way up.

Optionally, the function also takes care of plotting the algorithm and the histogram:

![][lane_lines]

The green rectangles represent the sliding window, the yellow lines the curves. All pixels that the algorithm categorizes as belonging to the left lane line are marked in red, the pixels belonging to the right lane lane are marked in blue. The red dots in the histogram indicate where the algorithm identified the base points for the left and right curve, respectively.

##### Optimized approach

The function `find_lane_lines_2(...)` in codel cell 14 is very similar, but takes advantage of know left and right fits from previous frames. It skips the histogram step, and uses the previous fits as "windows" (+/- a margin). The search areas are visualized in green here:

![][lane_lines_2]

#### 5. Describe how (and identify where in your code) you calculated the radius of curvature of the lane and the position of the vehicle with respect to center.

The curvature is calculated by the `calc_curve_rad_m(...)` function in code cell 16, whereas the offset is calculated by the `calc_offset(...)` function in code cell 18.

The former uses the `left_fit_m` and `right_fit_m` fits that were converted to meters before. The curvature is calculated at the bottom of the image.

The offset is simply the distance between the image center and the lane center, converted to meters.

#### 6. Provide an example image of your result plotted back down onto the road such that the lane area is identified clearly.

Code cells 20-22 take care of this.

The `augment_image(...)` function in cell 21 warps the image back to its original state using the inverse perspective transformation matrix, and adds a polygon fitting the two lane curves on top of it. With the `text_overlay(...)` function in cell 20, the curvature and offset information are added.

The result is seen here:

![][final]

---

### Pipeline (video)

#### 1. Provide a link to your final video output.  Your pipeline should perform reasonably well on the entire project video (wobbly lines are ok but no catastrophic failures that would cause the car to drive off the road!).

The video can be found in the folder as `output.mp4`.

Code cell 25 contains the final pipeline, that takes a frame from the video as input and returns an augmented image.

The first step is to process the image using the pre-pipeline, after which it is warped into bird's-eye view. Then, one of the two fitting algorithms is used to determine fits for both lane lines -- depending on whether previous (good!) fits are available.

The next step is a brief sanity check (cell 24) that checks if lane lines are separated by a reasonable distance in three vertical positions: at the bottom and top, as well as in the middle of the image. If one of these three distances exceeds the normal lane width (+/- margin), the sanity check fails.

If the fits pass the sanity check, they are added to a smoothing buffer of adjustable size, if they fail the check, they are ignored. If the sanity check is failed `insane_patience` times in a row, a reset is performed, and the pipeline starts with the histogram algorithm for the next frame.

Cells 26 and 27 initialize all variables and output the processed video.

The parameters used for the output video are (cell 23):
``` python
# Settings for smoothing
smoothing = True
smooth_buffer_size = 12

# Settings for sanity check
lane_width = 700
lane_width_deviation = 200
insane_patience = 5
```

---

### Discussion

#### 1. Briefly discuss any problems / issues you faced in your implementation of this project.  Where will your pipeline likely fail?  What could you do to make it more robust?

My first pipeline had issues with very light yellow lines and mistaking railings for lane lines (for example in the segment visible in `test7.jpg` and `test8.jpg`).

I was able to fix these issues by adjusting S-channel thresholds, window margins (and the `minpix` amount), and smoothing and sanity check settings. Additionally, I improved the histogram creation by slicing of the outmost parts, which stopped the algorithm from confusing railings for lane lines.

However, the pipeline is still not robust / flexible enough to pass either of the two challenge videos.

Three major improvements could be made:
1. A better *binary conversion* (including, but not limited to combining more thresholded channels and gradients)
2. A more robust *sliding window algorithm* (for example, let windows overlap when the algorithm otherwise fails to identify the curve in a satisfactory manner)
3. A more extensive *sanity check* that checks for curvature / parallelism

One could also simply try to adjust the thresholds and margins to specifically work for the challenge videos, since there is probably some room for improvement there.
